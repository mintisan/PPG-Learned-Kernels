{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f62bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #path/directory stuff\n",
    "import pickle\n",
    "\n",
    "#Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Math\n",
    "from sklearn.metrics import f1_score, r2_score\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "#Set seed for reproducibility\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" #device to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba0f56",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ed9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"PPG Data\" #Base directory for the PPG data\n",
    "subdirs = [ #sub dirs that contain each PPG dataset\n",
    "\"new_PPG_DaLiA_test/processed_dataset\",\n",
    "\"new_PPG_DaLiA_train/processed_dataset\",\n",
    "\"TROIKA_channel_1/processed_dataset\",\n",
    "\"WESAD_all/processed_dataset\"]\n",
    "\n",
    "#We use the DaLiA train set exclusively for training\n",
    "X_train = np.load(os.path.join(base, subdirs[1], \"scaled_ppgs.npy\"))\n",
    "Y_train = np.load(os.path.join(base, subdirs[1], \"seg_labels.npy\"))\n",
    "\n",
    "#The rest of these datasets are test\n",
    "DaLiA_X = np.load(os.path.join(base, subdirs[0], \"scaled_ppgs.npy\"))\n",
    "DaLiA_Y = np.load(os.path.join(base, subdirs[0], \"seg_labels.npy\"))\n",
    "\n",
    "TROIKA_X = np.load(os.path.join(base, subdirs[2], \"scaled_ppgs.npy\"))\n",
    "TROIKA_Y = np.load(os.path.join(base, subdirs[2], \"seg_labels.npy\"))\n",
    "\n",
    "WESAD_X = np.load(os.path.join(base, subdirs[3], \"scaled_ppgs.npy\"))\n",
    "WESAD_Y = np.load(os.path.join(base, subdirs[3], \"seg_labels.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b428bc",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1800aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedFilters(nn.Module):\n",
    "    def __init__(self, num_kernels=24):\n",
    "        super(LearnedFilters, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, num_kernels, 192, stride=1, padding=\"same\", bias=True)\n",
    "        self.conv2 = nn.Conv1d(1, num_kernels, 96, stride=1, padding=\"same\", bias=True)\n",
    "        self.conv3 = nn.Conv1d(1, num_kernels, 64, stride=1, padding=\"same\", bias=True)\n",
    "        \n",
    "        self.w1 = torch.nn.Parameter(torch.zeros(num_kernels), requires_grad=True) #these are learned weights for the kernels        \n",
    "        self.w2 = torch.nn.Parameter(torch.zeros(num_kernels), requires_grad=True)        \n",
    "        self.w3 = torch.nn.Parameter(torch.zeros(num_kernels), requires_grad=True)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        c1 = F.relu(F.relu(self.conv1(x))) * self.w1[None,:,None]\n",
    "        c2 = F.relu(F.relu(self.conv2(x))) * self.w2[None,:,None]\n",
    "        c3 = F.relu(F.relu(self.conv3(x))) * self.w3[None,:,None]\n",
    "        \n",
    "        aggregate = torch.cat([c1,c2,c3], dim=1)\n",
    "        aggregate = aggregate.sum(dim=1).view(batch_size, -1)\n",
    "        aggregate = torch.sigmoid(aggregate)\n",
    "        \n",
    "        return aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38a7c2",
   "metadata": {},
   "source": [
    "# Params and FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24ff874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LearnedFilters                           [1, 200]                  24\n",
       "├─Conv1d: 1-1                            [1, 8, 200]               1,544\n",
       "├─Conv1d: 1-2                            [1, 8, 200]               776\n",
       "├─Conv1d: 1-3                            [1, 8, 200]               520\n",
       "==========================================================================================\n",
       "Total params: 2,864\n",
       "Trainable params: 2,864\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.57\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.04\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = LearnedFilters(8)\n",
    "summary(model, input_size=(1, 1, 200))  # 第二个 1 是输入通道数，L 是输入数据的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f52fe2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "FLOPs:  281.600K\n",
      "Params:  2.840K\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "\n",
    "model = LearnedFilters(8)\n",
    "input = torch.randn(1, 1, 100)  # 假设 L 是输入数据的长度\n",
    "flops, params = profile(model, inputs=(input, ))\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print('FLOPs: ', flops)\n",
    "print('Params: ', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe835f51",
   "metadata": {},
   "source": [
    "# Memory peak usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4053c947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param memory for kernel size 192: 6176 bytes\n",
      "Param memory for kernel size 96: 3104 bytes\n",
      "Param memory for kernel size 64: 2080 bytes\n",
      "Activation memory for kernel size 192: 3200 bytes\n",
      "Activation memory for kernel size 96: 3200 bytes\n",
      "Activation memory for kernel size 64: 3200 bytes\n"
     ]
    }
   ],
   "source": [
    "num_kernels = 8  # 这是输出通道数\n",
    "input_channels = 1  # 假设输入通道数是1\n",
    "kernel_sizes = [192, 96, 64]  # 卷积核大小\n",
    "bias = True  # 假设每层都有偏置项\n",
    "L = 100\n",
    "\n",
    "# 计算每层的参数内存\n",
    "for kernel_size in kernel_sizes:\n",
    "    param_memory = (input_channels * kernel_size * num_kernels + (1 if bias else 0) * num_kernels) * 4\n",
    "    print(f\"Param memory for kernel size {kernel_size}: {param_memory} bytes\")\n",
    "\n",
    "# 假设输入长度L是固定的，我们计算激活内存\n",
    "# 注意：对于padding=\"same\"，输出长度与输入长度相同\n",
    "batch_size = 1  # 假设批处理大小为1\n",
    "for kernel_size in kernel_sizes:\n",
    "    output_height = L  # 因为padding=\"same\"，所以输出长度与输入长度相同\n",
    "    activation_memory = (batch_size * num_kernels * output_height) * 4\n",
    "    print(f\"Activation memory for kernel size {kernel_size}: {activation_memory} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c49a6d9",
   "metadata": {},
   "source": [
    "# Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee97573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "filter_nums = [4, 8, 16, 24, 32, 64, 128, 256, 512] #number of filters to train models for\n",
    "folds = 10 #number of folds to use for cross validation\n",
    "epochs = 512 #number of epochs to train for\n",
    "lr = 0.01 #learning rate\n",
    "wd = 1e-4 #weight decay\n",
    "decay_range = [1.0, 0.2] #range of decay values to use for learning rate decay\n",
    "\n",
    "save_dir = \"models\" #directory to save models to\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4264919a",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "Here, we train `folds` number of models for each `filter_nums` number of filters. Keep in mind that the end model will have `filter_num*3` filters, since the `filter_num` refers to the number of filters in each kernel group, for which there are three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54eaba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 1)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                               | 0/512 [00:00<?, ?it/s]C:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ..\\aten\\src\\ATen\\native\\Convolution.cpp:1004.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "Loss: 0.46818: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:11<00:00, 44.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 2)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.48722: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 3)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.47178: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:11<00:00, 43.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 4)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.48055: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 48.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 5)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.47213: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 6)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.47018: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 7)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.47113: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 8)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.48471: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 9)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.48903: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 4 filters (fold 10)...\n",
      "Num params: 1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.47044: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:10<00:00, 49.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 1)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.41477: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 2)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.41617: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 3)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.42443: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 4)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.41462: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 5)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.41369: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 6)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.41038: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 7)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.40829: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 8)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.41040: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 9)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.40628: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 8 filters (fold 10)...\n",
      "Num params: 2864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.40920: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:15<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 1)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36190: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:24<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 2)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36172: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 3)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36281: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 4)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36247: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 5)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36652: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 6)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36182: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 7)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.35831: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 8)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36156: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 9)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.36107: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 16 filters (fold 10)...\n",
      "Num params: 5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.37032: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:23<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 1)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33911: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 2)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34247: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 3)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34494: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 4)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34519: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 5)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34005: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 6)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34133: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 7)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34033: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 8)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34120: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 9)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34208: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 24 filters (fold 10)...\n",
      "Num params: 8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34527: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:33<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 1)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33372: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 2)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34001: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 3)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33291: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 4)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33125: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 5)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33229: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 6)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33425: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 7)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.34147: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 8)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33295: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 9)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33773: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 32 filters (fold 10)...\n",
      "Num params: 11456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.33226: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:52<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 1)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31684: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:38<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 2)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31501: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:38<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 3)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31586: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:38<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 4)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31711: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:38<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 5)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31695: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:40<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 6)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31693: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:40<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 7)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31610: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:40<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 8)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31451: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:40<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 9)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31741: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:40<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 64 filters (fold 10)...\n",
      "Num params: 22912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.31690: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [01:40<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 1)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30577: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:05<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 2)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30511: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 3)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30608: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 4)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30517: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:04<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 5)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30518: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:04<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 6)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30519: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:04<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 7)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30695: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 8)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30518: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:04<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 9)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30538: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:03<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 128 filters (fold 10)...\n",
      "Num params: 45824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30535: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [03:03<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 1)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30313: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:40<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 2)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30379: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:39<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 3)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30252: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:39<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 4)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30343: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:39<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 5)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30279: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:39<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 6)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30342: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:40<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 7)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30284: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:41<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 8)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30524: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:40<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 9)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30246: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:41<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 256 filters (fold 10)...\n",
      "Num params: 91648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30224: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [05:41<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 1)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.29977: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 2)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.29838: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:02<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 3)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30203: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:02<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 4)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.29964: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:02<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 5)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30056: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 6)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30180: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:00<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 7)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.29796: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:00<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 8)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.29820: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:00<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 9)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.29982: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:00<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kernel with 512 filters (fold 10)...\n",
      "Num params: 183296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.30033: 100%|██████████████████████████████████████████████████████████████████████████████████████| 512/512 [11:01<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through different numbers of filters\n",
    "for filter_num in filter_nums:\n",
    "    # Loop through different versions of the model\n",
    "    for fold in range(0, folds):\n",
    "        # Initialize a new instance of the LearnedFilters class with the current number of filters\n",
    "        net = LearnedFilters(filter_num).to(device)\n",
    "\n",
    "        # Compute the total number of model parameters and print it\n",
    "        params = sum([np.prod(p.size()) for p in filter(lambda p: p.requires_grad, net.parameters())])\n",
    "        print(f\"Training kernel with {filter_num} filters (fold {fold+1})...\")\n",
    "        print(\"Num params: %i\" % params)\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "        # Initialize a linear learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=decay_range[0], end_factor=decay_range[1], total_iters=epochs)\n",
    "\n",
    "        # Initialize a progress bar for visualization\n",
    "        pbar = tqdm(range(0, epochs))\n",
    "\n",
    "        # Normalize the input data by subtracting the mean and dividing by the standard deviation\n",
    "        x = copy.deepcopy(X_train) #we deepcopy the data so we don't modify the original\n",
    "        \n",
    "        #normalize each signal\n",
    "        for i in range(0, len(x)):\n",
    "            x[i] = (x[i] - np.mean(x[i]))/np.std(x[i])\n",
    "\n",
    "        # Convert the input and output data to PyTorch tensors and move them to the device\n",
    "        x = torch.tensor(x, dtype=torch.float32, device=device).reshape(len(x), 1, 1920)\n",
    "        y = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
    "\n",
    "        loss_hist = [] #history of losses (used to update the progress bar primarily)\n",
    "\n",
    "        # Train the model\n",
    "        for step in pbar:\n",
    "            # Zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialize the total loss to 0\n",
    "            total_loss = 0\n",
    "\n",
    "            # Split the input data into smaller batches if the number of filters is greater than 32 to avoid running out of memory\n",
    "            split = 32 if filter_num > 32 else 1\n",
    "            for i in range(0, split):\n",
    "                split_len = x.shape[0] // split\n",
    "                out = net(x[i*split_len:(i+1)*split_len]) # Forward pass\n",
    "                loss = F.binary_cross_entropy(out.view(-1), y[i*split_len:(i+1)*split_len].view(-1)) / split # Compute the loss\n",
    "                total_loss += loss.item() # Add the loss to the total loss\n",
    "                loss.backward() # Backward pass\n",
    "\n",
    "            optimizer.step() # Take an optimizer step\n",
    "            scheduler.step() # Adjust the learning rate\n",
    "\n",
    "            loss_hist.append(total_loss) # Add the total loss to the loss history\n",
    "            pbar.set_description(\"Loss: %.5f\" % np.mean(loss_hist[-256:])) # Update the progress bar with the current loss\n",
    "\n",
    "        # Save the model\n",
    "        torch.save(net, os.path.join(save_dir, f\"learned_filters_{filter_num}_{fold}.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0d317",
   "metadata": {},
   "source": [
    "# Test the models\n",
    "In this section, we simply run each model fold again the various test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b91424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(net, X, Y):\n",
    "    \"\"\"\n",
    "    Test the given network on the provided data.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The trained model.\n",
    "        X (list): The list of data samples.\n",
    "        Y (list): The list of corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        float: The DICE score of the model's predictions.\n",
    "    \"\"\"\n",
    "    dtype = net.state_dict()['w1'].dtype # Get the data type of the model\n",
    "    device = net.state_dict()['w1'].device # Get the device of the model\n",
    "    data = X\n",
    "    labels = Y\n",
    "\n",
    "    preds = [] # To store the model predictions\n",
    "    true = [] # To store the true labels\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        x = (x - np.mean(x))/np.std(x) # Normalize the data\n",
    "\n",
    "        y_pred = net(torch.tensor(x, dtype=dtype, device=device).view(1, -1)) # Predictions\n",
    "\n",
    "        # Apply a Savitzky-Golay filter to the predictions and convert to binary form\n",
    "        preds += list(np.float32(savgol_filter(y_pred.detach().cpu().float().numpy()[0], 151, 3) > 0.5)) \n",
    "        true += list(labels[i]) # Concatenate labels as list\n",
    "\n",
    "    return f1_score(preds, true) # Return the F1 score, which in this case, is equivalent to DICE score\n",
    "\n",
    "def test_suite(net, verbose=True):\n",
    "    \"\"\"\n",
    "    Test the given network on all the datasets and return and/or print the DICE scores\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The trained model.\n",
    "        verbose (bool, optional): Whether to print the DICE scores. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: The DICE scores for each dataset.\n",
    "    \"\"\"\n",
    "    results = [0, 0, 0] # Will store the F1 scores for each dataset\n",
    "\n",
    "    # Test on the DaLiA dataset\n",
    "    results[0] = test_model(net, DaLiA_X, DaLiA_Y)\n",
    "    if verbose:\n",
    "        print(\"DaLiA DICE score: %.4f\" % results[0])\n",
    "\n",
    "    # Test on the TROIKA dataset\n",
    "    results[1] = test_model(net, TROIKA_X, TROIKA_Y)\n",
    "    if verbose:\n",
    "        print(\"TROIKA DICE score: %.4f\" % results[1])\n",
    "\n",
    "    # Test on the WESAD dataset\n",
    "    results[2] = test_model(net, WESAD_X, WESAD_Y)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"WESAD DICE score: %.4f\" % results[2])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a06ac3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 4 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8190, TROIKA: 0.6892, WESAD: 0.8639: 100%|█████████████████████████████████████████████████████████| 10/10 [02:29<00:00, 14.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 8 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8429, TROIKA: 0.7320, WESAD: 0.8814: 100%|█████████████████████████████████████████████████████████| 10/10 [02:31<00:00, 15.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 16 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8570, TROIKA: 0.7896, WESAD: 0.8928: 100%|█████████████████████████████████████████████████████████| 10/10 [02:30<00:00, 15.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 24 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8610, TROIKA: 0.8066, WESAD: 0.8969: 100%|█████████████████████████████████████████████████████████| 10/10 [02:32<00:00, 15.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 32 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8623, TROIKA: 0.8124, WESAD: 0.8976: 100%|█████████████████████████████████████████████████████████| 10/10 [02:29<00:00, 14.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 64 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8664, TROIKA: 0.8259, WESAD: 0.9024: 100%|█████████████████████████████████████████████████████████| 10/10 [02:33<00:00, 15.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 128 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8692, TROIKA: 0.8324, WESAD: 0.9050: 100%|█████████████████████████████████████████████████████████| 10/10 [02:50<00:00, 17.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 256 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8698, TROIKA: 0.8317, WESAD: 0.9061: 100%|█████████████████████████████████████████████████████████| 10/10 [02:49<00:00, 16.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing kernel with 512 filters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DaLiA: 0.8709, TROIKA: 0.8303, WESAD: 0.9076: 100%|█████████████████████████████████████████████████████████| 10/10 [02:47<00:00, 16.74s/it]\n"
     ]
    }
   ],
   "source": [
    "test_results = {} #will store the results of each model\n",
    "\n",
    "# Loop through different numbers of filters\n",
    "for filter_num in filter_nums:\n",
    "    print(f\"Testing kernel with {filter_num} filters...\")\n",
    "    # Loop through different versions of the model\n",
    "    pbar = tqdm(range(0, folds))\n",
    "    results = []\n",
    "    for fold in pbar: #loop through all the folds\n",
    "        # Load the model\n",
    "        net = torch.load(os.path.join(save_dir, f\"learned_filters_{filter_num}_{fold}.pt\"), map_location=device)\n",
    "\n",
    "        # Test the model\n",
    "        results.append(test_suite(net, verbose=False))\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.set_description(f\"DaLiA: %.4f, TROIKA: %.4f, WESAD: %.4f\" % tuple(np.mean(results, axis=0)))\n",
    "    \n",
    "    test_results[filter_num] = np.transpose(results) #transpose the results so that the rows are the datasets and the columns are the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2e06a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results\n",
    "with open(os.path.join(save_dir, \"test_results.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ec078",
   "metadata": {},
   "source": [
    "# Kernel Pruning\n",
    "This code removes similar kernels from each model and tests the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b25bd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8693\n",
      "TROIKA DICE score: 0.8359\n",
      "WESAD DICE score: 0.9048\n",
      "\n",
      "Removed 11.01% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8630 (99.28% of original)\n",
      "TROIKA DICE score: 0.8281 (99.07% of original)\n",
      "WESAD DICE score: 0.9001 (99.48% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8692\n",
      "TROIKA DICE score: 0.8326\n",
      "WESAD DICE score: 0.9050\n",
      "\n",
      "Removed 12.28% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8652 (99.54% of original)\n",
      "TROIKA DICE score: 0.8288 (99.54% of original)\n",
      "WESAD DICE score: 0.9018 (99.65% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8686\n",
      "TROIKA DICE score: 0.8332\n",
      "WESAD DICE score: 0.9048\n",
      "\n",
      "Removed 11.85% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8639 (99.46% of original)\n",
      "TROIKA DICE score: 0.8318 (99.84% of original)\n",
      "WESAD DICE score: 0.8961 (99.03% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8695\n",
      "TROIKA DICE score: 0.8352\n",
      "WESAD DICE score: 0.9052\n",
      "\n",
      "Removed 11.43% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8639 (99.37% of original)\n",
      "TROIKA DICE score: 0.8135 (97.41% of original)\n",
      "WESAD DICE score: 0.9011 (99.55% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8692\n",
      "TROIKA DICE score: 0.8280\n",
      "WESAD DICE score: 0.9049\n",
      "\n",
      "Removed 12.28% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8633 (99.32% of original)\n",
      "TROIKA DICE score: 0.8342 (100.75% of original)\n",
      "WESAD DICE score: 0.8974 (99.17% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8694\n",
      "TROIKA DICE score: 0.8356\n",
      "WESAD DICE score: 0.9053\n",
      "\n",
      "Removed 12.70% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8658 (99.59% of original)\n",
      "TROIKA DICE score: 0.8302 (99.36% of original)\n",
      "WESAD DICE score: 0.9000 (99.42% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8690\n",
      "TROIKA DICE score: 0.8299\n",
      "WESAD DICE score: 0.9045\n",
      "\n",
      "Removed 13.55% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8650 (99.54% of original)\n",
      "TROIKA DICE score: 0.8352 (100.64% of original)\n",
      "WESAD DICE score: 0.9000 (99.50% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8694\n",
      "TROIKA DICE score: 0.8299\n",
      "WESAD DICE score: 0.9052\n",
      "\n",
      "Removed 11.43% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8637 (99.34% of original)\n",
      "TROIKA DICE score: 0.8333 (100.41% of original)\n",
      "WESAD DICE score: 0.8978 (99.18% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8695\n",
      "TROIKA DICE score: 0.8314\n",
      "WESAD DICE score: 0.9050\n",
      "\n",
      "Removed 11.43% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8554 (98.38% of original)\n",
      "TROIKA DICE score: 0.8298 (99.81% of original)\n",
      "WESAD DICE score: 0.8915 (98.52% of original)\n",
      "=====================================\n",
      "-------Before pruning-------\n",
      "DaLiA DICE score: 0.8689\n",
      "TROIKA DICE score: 0.8322\n",
      "WESAD DICE score: 0.9050\n",
      "\n",
      "Removed 13.12% of params\n",
      "-------After pruning-------\n",
      "DaLiA DICE score: 0.8620 (99.21% of original)\n",
      "TROIKA DICE score: 0.8278 (99.46% of original)\n",
      "WESAD DICE score: 0.9001 (99.46% of original)\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "def similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        v1, v2 (torch.Tensor): The input vectors.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between v1 and v2.\n",
    "    \"\"\"\n",
    "    norm_v1 = v1 / v1.norm()\n",
    "    norm_v2 = v2 / v2.norm()\n",
    "    \n",
    "    return (norm_v1*norm_v2).sum().item()\n",
    "\n",
    "def compute_param_num(num_conv1, num_conv2, num_conv3):\n",
    "    \"\"\"\n",
    "    Compute the number of parameters in a network given the number of kernels in each layer.\n",
    "\n",
    "    Args:\n",
    "        num_conv1, num_conv2, num_conv3 (int): The number of kernels in each convolutional layer.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of parameters in the network.\n",
    "    \"\"\"\n",
    "    params = num_conv1*192 +num_conv2*96 + num_conv3*64 #kernel params\n",
    "    params += num_conv1 + num_conv2 + num_conv3 #biases (1 per kernel)\n",
    "    params += num_conv1 + num_conv2 + num_conv3 #weights (1 per kernel)\n",
    "    \n",
    "    return params\n",
    "\n",
    "def get_most_similar_kernels(similarity_flat, coords):\n",
    "    \"\"\"\n",
    "    Get the indices of the most similar kernels based on their similarity scores.\n",
    "\n",
    "    Args:\n",
    "        similarity_flat (np.array): The flattened array of similarity scores.\n",
    "        coords (np.array): The flattened array of kernel index pairs.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The indices of the most similar kernels.\n",
    "    \"\"\"\n",
    "    return coords[np.argsort(similarity_flat)]\n",
    "\n",
    "\n",
    "def compute_similarity(state_dict, conv_i, num_kernels):\n",
    "    \"\"\"\n",
    "    Compute the similarity between convolutional kernels for a given layer.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state dict of the network.\n",
    "        conv_i (int): Index of the convolutional layer.\n",
    "        num_kernels (int): Number of kernels in each layer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two numpy arrays containing the flattened similarity scores and their corresponding coordinates.\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    similarity_flat = []\n",
    "    \n",
    "    # Iterate over all pairs of kernels\n",
    "    for i in range(num_kernels):\n",
    "        for j in range(i, num_kernels):\n",
    "            if i != j:\n",
    "                sim = similarity(state_dict[f'conv{conv_i}.weight'][i], state_dict[f'conv{conv_i}.weight'][j])\n",
    "                similarity_flat.append(sim)\n",
    "                coords.append((j, i))\n",
    "\n",
    "    return np.asarray(similarity_flat), np.asarray(coords)\n",
    "\n",
    "\n",
    "def prune(state_dict, conv_i, num_kernels, prune_ratio):\n",
    "    \"\"\"\n",
    "    Prune the least important kernels from a kernel group based on cosine similarity and kernel importance.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state dict of the network.\n",
    "        conv_i (int): Index of the kernel group.\n",
    "        num_kernels (int): Number of kernels in each group.\n",
    "        prune_ratio (float): The proportion of kernels to prune.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state dict after pruning.\n",
    "    \"\"\"\n",
    "    # Compute similarity of kernels\n",
    "    sim_flat, coords = compute_similarity(state_dict, conv_i, num_kernels)\n",
    "    \n",
    "    # Get the most similar kernels\n",
    "    most_similar_kernels = get_most_similar_kernels(sim_flat, coords)\n",
    "\n",
    "    # Prune if the ratio is greater than zero, otherwise do nothing\n",
    "    if prune_ratio > 0:\n",
    "        # Iterate over the most similar kernels\n",
    "        for item in most_similar_kernels[-int(num_kernels*prune_ratio):]:\n",
    "            # Calculate weights for two kernels under consideration\n",
    "            item0_weight = state_dict[f'w{conv_i}'][item[0]]*state_dict[f'conv{conv_i}.weight'][item[0]].abs().mean()\n",
    "            item1_weight = state_dict[f'w{conv_i}'][item[1]]*state_dict[f'conv{conv_i}.weight'][item[1]].abs().mean()\n",
    "\n",
    "            # Decide which kernel to remove and which to keep\n",
    "            remove, keep, keep_weight, remove_weight = (item[1], item[0], item0_weight, item1_weight) if item0_weight > item1_weight else (item[0], item[1], item1_weight, item0_weight)\n",
    "            \n",
    "            # Update state_dict\n",
    "            state_dict[f'w{conv_i}'][keep] = (keep_weight + remove_weight) / state_dict[f'conv{conv_i}.weight'][keep].abs().mean()\n",
    "            state_dict[f'conv{conv_i}.bias'][keep] += state_dict[f'conv{conv_i}.bias'][remove]\n",
    "            \n",
    "            # this step is actually enough to \"prune the kernel\", for computation/measurement sake. In reality, we'd want to remove the kernel from the network for a speedup\n",
    "            state_dict[f'w{conv_i}'][remove] = 0.0\n",
    "            state_dict[f'conv{conv_i}.bias'][remove] = 0.0\n",
    "\n",
    "            # this is an extra step used for counting the kernel that we remove in the end\n",
    "            # no matter what this value is set to, it will not have any effect on the network since the weight is set to zero\n",
    "            # however, this value *does* need to be non-zero, since PyTorch handles completely zeroed convolutions a bit weirdly\n",
    "            # and you'll get weird results convolving a purely 0 kernel\n",
    "            state_dict[f'conv{conv_i}.weight'][remove] = 1e-5\n",
    "            \n",
    "    return state_dict\n",
    "\n",
    "def prune_network(net, num_kernels, prune_ratio):\n",
    "    \"\"\"\n",
    "    Prune the least important kernels from the model.\n",
    "\n",
    "    Args:\n",
    "        net (nn.Module): The model.\n",
    "        num_kernels (int): Number of kernels in each group.\n",
    "        prune_ratio (list): The proportion of kernels to prune in each group.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The pruned network.\n",
    "    \"\"\"\n",
    "    state_dict = net.state_dict()\n",
    "\n",
    "    # Iterate over all layers and prune\n",
    "    for conv_i in range(1, 4):\n",
    "        state_dict = prune(state_dict, conv_i, num_kernels, prune_ratio[conv_i-1])\n",
    "    net.load_state_dict(state_dict)\n",
    "    return net\n",
    "\n",
    "\n",
    "def count_nonzero_weights(state_dict, num_kernels):\n",
    "    \"\"\"\n",
    "    Count the number of non-zero weights in each kernel group of the model.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state dict of the network.\n",
    "        num_kernels (int): Number of kernels in each layer.\n",
    "\n",
    "    Returns:\n",
    "        list: The number of non-zero weights in each layer.\n",
    "    \"\"\"\n",
    "    zero_weights = [0, 0, 0]\n",
    "\n",
    "    # Iterate over all layers and kernels\n",
    "    for i in range(1, 4):\n",
    "        for j in range(0, num_kernels):\n",
    "            # Count the zero weights\n",
    "            if (state_dict[f'conv{i}.weight'][j] == 1e-5).all(): #this is the value we set the weights to in the prune function, it is arbitrary\n",
    "                zero_weights[i-1] += 1\n",
    "    nonzero_weights = [num_kernels - zero_weights[i] for i in range(3)]\n",
    "\n",
    "    return nonzero_weights\n",
    "\n",
    "\n",
    "# Initiate lists to store results\n",
    "pre_prunes = [] # DICE scores before pruning\n",
    "post_prunes = [] # DICE scores after pruning\n",
    "reductions = [] # Reduction in parameters\n",
    "num_kernels = 128 # Number of kernels in each kernel group\n",
    "\n",
    "# Iterate over models and prune\n",
    "for j in range(folds):\n",
    "    # Load the network\n",
    "    net = torch.load(f\"models/learned_filters_{num_kernels}_{j}.pt\", map_location=device)\n",
    "\n",
    "    print(\"-------Before pruning-------\")\n",
    "    # Test the network before pruning\n",
    "    pre_prune = test_suite(net, verbose=True)\n",
    "    pre_prunes.append(pre_prune)\n",
    "\n",
    "    # Define the pruning ratio for each layer, essentially, this is the proportion of kernel *pairs* for which one pair will be removed\n",
    "    # In other words, if the pruning ratio is 1.0, then *at most* half of all the kernels for that layer will be removed\n",
    "    # However, it is not *guaranteed* that half will be removed, since the similarity ordering can cause some kernels to be the most similar to multiple other kernels\n",
    "    # thus, this kernel could be removed first, leading to the other pairs to have \"already been pruned\".\n",
    "    # This is also why we have to manually compute the number of parameters removed\n",
    "    prune_ratio = [0.35, 0.0, 0.0]\n",
    "    \n",
    "    # Prune the network\n",
    "    net = prune_network(net, num_kernels, prune_ratio)\n",
    "\n",
    "    # Count the number of non-zero weights\n",
    "    nonzero_weights = count_nonzero_weights(net.state_dict(), num_kernels)\n",
    "    new_kernel_num = nonzero_weights\n",
    "\n",
    "    # Compute the new parameter count and the reduction\n",
    "    new_param_count = compute_param_num(new_kernel_num[0], new_kernel_num[1], new_kernel_num[2])\n",
    "    reduction_percentage = (1 - new_param_count/compute_param_num(num_kernels, num_kernels, num_kernels))*100\n",
    "    reductions.append(reduction_percentage)\n",
    "\n",
    "    print(f\"\\nRemoved {reduction_percentage:.2f}% of params\")\n",
    "    print(\"-------After pruning-------\")\n",
    "\n",
    "    # Test the network after pruning\n",
    "    post_prune = test_suite(net, verbose=False)\n",
    "    post_prunes.append(post_prune)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"DaLiA DICE score: {post_prune[0]:.4f} ({post_prune[0]/pre_prune[0]*100:.2f}% of original)\")\n",
    "    print(f\"TROIKA DICE score: {post_prune[1]:.4f} ({post_prune[1]/pre_prune[1]*100:.2f}% of original)\")\n",
    "    print(f\"WESAD DICE score: {post_prune[2]:.4f} ({post_prune[2]/pre_prune[2]*100:.2f}% of original)\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5802034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
